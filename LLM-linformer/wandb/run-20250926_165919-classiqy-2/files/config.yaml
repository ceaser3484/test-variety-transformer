_wandb:
    value:
        cli_version: 0.21.3
        e:
            u2tpyky840s1pesj59nd0x7qusqc6jr5:
                codePath: LLM-linformer/main.py
                codePathLocal: main.py
                cpu_count: 16
                cpu_count_logical: 24
                cudaVersion: "12.8"
                disk:
                    /:
                        total: "982240026624"
                        used: "123340668928"
                email: ceaser3484@gmail.com
                executable: /home/ceaser/miniconda3/envs/pytorch-ubuntu/bin/python
                git:
                    commit: eb392d102e383e93707fad802960acc1b02355ea
                    remote: https://github.com/ceaser3484/test-variety-transformer.git
                gpu: NVIDIA GeForce RTX 3060
                gpu_count: 1
                gpu_nvidia:
                    - architecture: Ampere
                      cudaCores: 3584
                      memoryTotal: "12884901888"
                      name: NVIDIA GeForce RTX 3060
                      uuid: GPU-dd9b63a5-03a9-dc6a-7a60-c9da769d4c1c
                host: ubuntu-ai
                memory:
                    total: "50353807360"
                os: Linux-6.14.0-29-generic-x86_64-with-glibc2.39
                program: /home/ceaser/PycharmProject/test-variety-transformer/LLM-linformer/main.py
                python: CPython 3.12.0
                root: /home/ceaser/PycharmProject/test-variety-transformer/LLM-linformer
                startedAt: "2025-09-26T07:59:19.702830Z"
                writerId: u2tpyky840s1pesj59nd0x7qusqc6jr5
        m: []
        python_version: 3.12.0
        t:
            "1":
                - 1
                - 5
                - 53
            "2":
                - 1
                - 5
                - 53
            "3":
                - 13
                - 14
                - 16
            "4": 3.12.0
            "5": 0.21.3
            "12": 0.21.3
            "13": linux-x86_64
attention_dropout:
    value: 0.5
batch_size:
    value: 16
embedding_dropout:
    value: 0.2
ffn_dropout:
    value: 0.5
head_dim:
    value: 32
k:
    value: 32
learning_rate:
    value: 0.0001
max_len:
    value: 800
n_head:
    value: 1
n_stack:
    value: 1
num_epochs:
    value: 20
num_fold:
    value: 6
patience:
    value: 5
scheduler_gamma:
    value: 0.8
scheduler_step_size:
    value: 10
shuffle:
    value: true
weight_decay:
    value: 0.03
